{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Dairo Alberto Cuervo Garcia\n","#### Proyecto de grado - Exploración de discurso en cartas de maximos responsables - Maestría en Ciencias de los Datos y Analítica - Universidad EAFIT - 2024/1"]},{"cell_type":"markdown","metadata":{},"source":["## Preparación de texto y preprocesamiento"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\DairoAlbertoCuervoGa\\AppData\\Roaming\\nltk_dat\n","[nltk_data]     a...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\DairoAlbertoCuervoGa\\AppData\\Roaming\\nltk_dat\n","[nltk_data]     a...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to\n","[nltk_data]     C:\\Users\\DairoAlbertoCuervoGa\\AppData\\Roaming\\nltk_dat\n","[nltk_data]     a...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\DairoAlbertoCuervoGa\\AppData\\Roaming\\nltk_dat\n","[nltk_data]     a...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Cargar librerias necesarias\n","import pandas as pd\n","import numpy as np\n","import re\n","import string\n","import nltk\n","import spacy\n","from nltk.tokenize import casual_tokenize\n","from nltk.corpus import stopwords\n","from unidecode import unidecode\n","from spacy.lang.es import Spanish\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","# Dependencias de NLTK\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["ruta_archivo_csv = r'C:\\Users\\DairoAlbertoCuervoGa\\Downloads\\Text_project\\1.1.Output\\Resultados.csv' ## Cargar archivo csv\n","df = pd.read_csv(ruta_archivo_csv)"]},{"cell_type":"markdown","metadata":{},"source":["### Limpiar datos\n","Se realiza una serie de pasos dentro de una función para limpiar texto antes de preprocesamiento"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Crear función de limpieza de texto\n","def limpiar_texto(texto):\n","    texto = re.sub(r'-', ' ', texto) # Reemplazar guiones por espacios vacíos\n","    texto = texto.lower() # Convertir a minúsculas\n","    texto = re.sub(r'\\d+', '', texto) # Eliminar números y puntuaciones\n","    texto = texto.translate(str.maketrans(\"\", \"\", string.punctuation))\n","    texto = re.sub(r'[^\\w\\s]', '', texto) # Eliminar caracteres especiales\n","    texto = re.sub(r'\\s+', ' ', texto) # Eliminar espacios adicionales\n","    texto = re.sub(r'[\\n\\r]', ' ', texto) #Eliminar los saltos de línea (\\n) y retornos de carro (\\r) con la expresión regular r'[\\n\\r]'.\n","    texto = re.sub(r'\\$\\d+\\.?\\d*', '', texto) #Eliminar los símbolos de moneda con la expresión regular r'\\$\\d+\\.?\\d*' (para el caso de dólares). \n","    texto = re.sub(r'\\b([MDCLXVI]+)\\b', '', texto) #Eliminar los números romanos con la expresión regular r'\\b([MDCLXVI]+)\\b'.\n","    texto = re.sub(r'\\b\\w\\b', '', texto) #Eliminar las palabras que contienen solo una letra (como \"a\", \"e\", \"o\") con la expresión regular r'\\b\\w\\b'.\n","    texto = reemplazar_comillas(texto)  #agregar función de reemplazar comillas dobles.\n","    return texto\n","\n","# Función para reemplazar las comillas dobles por nada.\n","def reemplazar_comillas(texto):\n","    texto = texto.replace('\"', '')\n","    return texto\n","\n","df['texto_limpio'] = df['Texto'].apply(limpiar_texto)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenización, lematización y remoción de stopwords"]},{"cell_type":"markdown","metadata":{},"source":["#### Creación de listado de palabras a eliminar\n","- Se crea un listado de palabras a eliminar que limpia los textos procesados para la mejora de modelos, toma palabras de baja relevancia y con alta frecuencia"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["palabras_eliminar = ['millón','millon','carta','presidente','año','pais','billón', 'colombia', 'país','inmediatamente','anterior', 'él','el','grupo nutresa',\n","                     'grupo éxito','éxito','grupo', 'grupo exito','bavaria','grupo aval',\n","                     'bancolombia','corficolombiana','banco bogota', 'banco bogotá','banco de bogota', \n","                     'banco de bogotá','davivienda','cementos argos', 'cemex', 'grupo argos','ecopetrol',\n","                     'isa','canacol','frontera energy','drummond','cerrejon','reficar','cenit','celsia',\n","                     'grupo de energia de bogotá', 'grupo de energia de bogota','grupo energia bogotá', \n","                     'grupo energia bogota','sura','epm','claro','avianca','aval','bolívar','banco','bancos','daviplata','nutresa','argos'] \n","nlp = spacy.load('es_core_news_sm') #palabras a eliminar\n","nlp.max_length = 1500000\n"]},{"cell_type":"markdown","metadata":{},"source":["##### Tokenización"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["stop_words_nltk = set(stopwords.words('spanish'))  # cargar stopwords en nltk"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nK9rKXWvyHXn"},"outputs":[],"source":["# Función que tokenize los textos\n","def tokenizar(text):\n","    tokens = casual_tokenize(str(text)) #retirar cambio de guiones por espacios vacíos y agregarlo en la preparación\n","    #tokens = nltk.tokenize.casual_tokenize(str(text))\n","    tokens = [w.lower() for w in tokens if len(w) > 3] #excluir tokens iguales o inferiores a 3 letras\n","    #tokens = [unidecode(w) for w in tokens] # activar para quitar tildes (no recomendado se deja comentado por que afecta los resultados)\n","    tokens = [w for w in tokens if w not in stop_words_nltk]\n","    tokens = [w for w in tokens if w.isalpha()]\n","    tokens = [w for w in tokens if w.isalpha() and w not in palabras_eliminar] #se agrega eliminar palabras especificas\n","    return \" \".join(tokens)\n","\n","df['texto_tokenizado'] = df['texto_limpio'].apply(tokenizar)"]},{"cell_type":"markdown","metadata":{},"source":["##### Lematización y remoción de stopwords\n","- Remoción de stopwords usando NLTK en español\n","- Lematización"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Crear función de lematizado y remoción de stop words\n","def lematizar_texto(texto):\n","    doc = nlp(texto)\n","    tokens_sin_stopwords = [token.lemma_ for token in doc if not token.is_stop and token.lemma_ not in palabras_eliminar] #agregar eliminar palabras especificas\n","    return \" \".join(tokens_sin_stopwords)\n","\n","df['texto_lematizado'] = df['texto_tokenizado'].apply(lematizar_texto)"]},{"cell_type":"markdown","metadata":{},"source":["#### Anexar a consulta y almacenar"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Agregar la columna de empresa como índice del dataframe\n","df['Empresa_Año'] = df['Empresa'] + '_' + df['Año'].astype(str)\n","df.set_index('Empresa_Año', inplace=True)\n","\n","# Visualizar los primeros registros del DataFrame\n","#df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Guardar el DataFrame procesado en un nuevo archivo CSV\n","ruta_salida_csv = r'C:\\Users\\DairoAlbertoCuervoGa\\Downloads\\Text_project\\1.1.Output\\Resultados.csv'\n","df.to_csv(ruta_salida_csv)\n","\n","# Visualizar los primeros registros del DataFrame\n","#df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Cálculo de número de condición\n","Este número ayuda a detectar la sensibilidad de las operaciones matem ́aticas frente a errores de redondeo,\n","se observa que a medida que se mejora la limpieza de los textos, el n umerode condición disminuye, este apartado tiene el resultado final, en el Anexo_escenarios_iniciales se recrea brevemente los resultados sin los pasos de procesamiento adecuado."]},{"cell_type":"markdown","metadata":{},"source":["#### Creación de matriz TF-IDF\n","Indexación y creación de matriz de similitud por métrica del coseno"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Vectorización TF-IDF\n","vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='utf-8', ngram_range=(1, 1)) #solo se procesan ngram=1\n","tf_idf_matrix = vectorizer.fit_transform(df['texto_lematizado'])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Crear matriz de similitud\n","similarity_matrix = pd.DataFrame(cosine_similarity(tf_idf_matrix), index=df.index, columns=df.index)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["El número de condición de la matriz de similitud es: 85.77500800225008\n"]}],"source":["# Calcular el número de condición de la matriz de similitud\n","cond_A = np.linalg.cond(similarity_matrix)\n","\n","# Imprimir el resultado\n","print(\"El número de condición de la matriz de similitud es:\", cond_A)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1OYYjVefdpax3au1YVxx4LGAzmPj_H_0F","timestamp":1683389349641},{"file_id":"1P-FMF0npo1iTK_tjA-FxOsrLgcbV5l5C","timestamp":1683086633354}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
